{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Feature enginerring,slection and scaling.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1RUm3X96o3LvOXHoTvnKcDZlSezP76_yu","authorship_tag":"ABX9TyOXomDNXkWctOM9JlELijqF"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["\n","In this file i remove duplicate values and filled na values \n","\n","merged categoreies of some features\n","\n","statistical tests \n","\n","one hot encoding\n","\n","normalization\n","\n","\n","\n"],"metadata":{"id":"RChSqqT5fyXg"}},{"cell_type":"code","source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","pd.set_option('display.max_rows', None)\n","pd.set_option('display.max_columns',None)"],"metadata":{"id":"vVNNEmjgfvLu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# reading the file\n","path = \"/content/drive/MyDrive/adult.csv\"\n","df = pd.read_csv(path)"],"metadata":{"id":"7Cb4CCGbUfsO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["output_var = df['income']"],"metadata":{"id":"YAoBsi4uUx6z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["we have checked in eda that we have duplicated rows present in our data so i am gonna drop them"],"metadata":{"id":"Z2jTnSDmYuPg"}},{"cell_type":"code","source":["# checking duplicated data \n","\n","# check the shape\n","print(f\"we have {df.shape[0]} rows and {df.shape[1]} columns\")\n","\n","duplicate_data = df.duplicated()\n","print(f\"we have {duplicate_data.sum()} duplicate data\")\n","\n","# we have duplicate data,removing them from our data\n","df.drop_duplicates(inplace = True)\n","\n","# check the shape again\n","print(f\"we have {df.shape[0]} rows and {df.shape[1]} columns\")\n","\n","# checking duplicated data again\n","duplicate_data = df.duplicated()\n","print(f\"we have {duplicate_data.sum()} duplicate data\")"],"metadata":{"id":"gFxkSKmzW_GG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["we have checked in eda that we have question mark in three columns which is workclass,occupation and native.country , so all these three are nominal categorical variable so i am gonna create new label for the missing values named by 'missing'"],"metadata":{"id":"1jLzeK1SmxhJ"}},{"cell_type":"code","source":["# filling the question marks with missing label in data\n","df['workclass'].replace(\"?\",'Missing',inplace = True)\n","df['occupation'].replace(\"?\",'Missing',inplace = True)\n","df['native.country'].replace(\"?\",'Missing',inplace = True)"],"metadata":{"id":"KRnuiD9Ym8xR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# saving csv file in pands with filled na values \n","# df.to_csv('/content/drive/My Drive/adult_filled_na.csv',index=False)"],"metadata":{"id":"yKoyqVAdeTMb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# rechecking the question mark in data\n","question_mark = \"?\"\n","question_mark = df.isin([question_mark]).sum()\n","print(question_mark)"],"metadata":{"id":"aHOQJXdFnHnC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**merging categories of some features**\n","\n","In  Workclasss, merging Never-worked and Without- pay into one category which  is \"Neverworked/withoutpay\"\n","\n","In race, merging Asian-Pac-Islander and Amer-Indian-Eskimo and other into \"other_race\"\n","\n","In native.country, Except United states merging all ther countries into \"other_countries\"\n","\n","Education and education.num are overlapping so we will delete education column.\n","\n","education_dict = {\"preschool\" :1, \"1st-4th\": 2, \"5th-6th\": 3, \"7th-8th\": 4, \"9th\": 5, \"10th\": 6, \"11th\": 7, \"12th\": 8, \"HS-grad\": 9, \"Some-college\": 10, \"Assoc-voc\": 11, \"Assoc-acdm\": 12, \"Bachelors\": 13, \"Masters\": 14,\"Prof-school\": 15, \"Doctorate\": 16}\n","\n","In education.num,merginng preschool to 12th into \"Higher_secondary\". "],"metadata":{"id":"RxF58hzkIwP6"}},{"cell_type":"code","source":["# merging Never-worked and Without- pay into one category which is \"Neverworked/withoutpay\"\n","df['workclass'].replace('Without-pay','Neverworked/withoutpay',inplace = True)\n","df['workclass'].replace('Never-worked','Neverworked/withoutpay',inplace = True)\n","\n","#merging Asian-Pac-Islander and Amer-Indian-Eskimo and other into \"other_race\"\n","df['race'].replace('Asian-Pac-Islander','Other_race',inplace = True)\n","df['race'].replace('Amer-Indian-Eskimo','Other_race',inplace = True)\n","df['race'].replace('Other','Other_race',inplace = True)\n","\n","# Except United states merging all ther countries into \"other_country\n","df['native.country'].where(~(df['native.country'] !=  'United-States'), other = \"Other_country\",inplace = True)\n","\n","#In education.num,merginng preschool to 12th into \"Higher_secondary\". \n","df['education.num'].replace([1,2,3,4,5,6,7,8],1,inplace = True)\n","df['education.num'].replace(9,2,inplace = True)\n","df['education.num'].replace(10,3,inplace = True)\n","df['education.num'].replace(11,4,inplace = True)\n","df['education.num'].replace(12,5,inplace = True)\n","df['education.num'].replace(13,6,inplace = True)\n","df['education.num'].replace(14,7,inplace = True)\n","df['education.num'].replace(15,8,inplace = True)\n","df['education.num'].replace(16,9,inplace = True)"],"metadata":{"id":"vIuT7OEMO5vn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#dropping some columns(education)\n","df.drop(['education'],axis = 1,inplace = True)"],"metadata":{"id":"mcWo9QUHQhS2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Feature Selection by statistical test**\n"],"metadata":{"id":"68ocKX6L3YGZ"}},{"cell_type":"code","source":["from scipy.stats import ttest_ind \n","\n","def ttest(df,var1):\n","  cat1 = df.query(\"income == '>50K'\")\n","  cat2 = df.query(\"income == '<=50K'\")\n","\n","  stat,p = ttest_ind(cat1[var1], cat2[var1])\n","\n","   # interpret p-value\n","  alpha = 0.05\n","  print(f\"\\nalpha is {alpha} , p value is {p}\")\n","  if p <= alpha:\n","\t  print('\\nreject null hypothesis,there is relationship between variables')\n","  else:\n","\t  print('\\nfail to reject null hypothesis,there is no relationship between variables')\n","   \n","  return"],"metadata":{"id":"T2Va7Uea3SoB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# continuous_var = ['age','fnlwgt', 'capital.gain','capital.loss', 'hours.per.week']\n","ttest(df,'capital.loss')"],"metadata":{"id":"vKGLD-IX3TBL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["age = p value is 0.0\n","\n","fnlwgt = p value is 0.08652724317860144\n","\n","capital.gain = p value is 0.0\n","\n","capital.loss = p value is 4.022159409210232e-164\n","\n","hours.per.week = p value is 0.0\n","\n","By analyzing p values, Except fnlwgt i will take all variable for normalization"],"metadata":{"id":"lL0E9kFL32g_"}},{"cell_type":"code","source":["# chi-squared test \n","\n","from scipy.stats import chi2_contingency\n","from scipy.stats import chi2\n","\n","def chi_squared_test(df,var1,var2):\n","  df_table = pd.crosstab(df[var1],df[var2])\n","  print(df_table)\n","  \n","  stat, p, dof, expected = chi2_contingency(df_table)\n","  print(\"\\nthe degree of freedom is :\",dof)\n","  print(\"\\nthe expected value is \\n\",expected)\n","  \n","  # interpret p-value\n","  alpha = 0.05\n","  print(f\"\\nalpha is {alpha} , p value is {p}\")\n","  if p <= alpha:\n","\t  print('\\nreject null hypothesis,there is relationship between variables')\n","  else:\n","\t  print('\\nfail to reject null hypothesis,there is no relationship between variables')\n","   \n","  return"],"metadata":{"id":"Dlclq9VH3t3w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#categ_var = ['sex','workclass','marital.status','occupation','relationship','race','native.country','education.num']\n","\n","chi_squared_test(df,'income','education.num')"],"metadata":{"id":"eWCdX2ok4j7A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["sex = p value is 0.0\n","\n","workclass = p value is 2.6455448138696704e-221\n","\n","marital.status = p value is 0.0\n","\n","occupation = p value is 0.0\n","\n","relationship = p value is 0.0\n","\n","race = p value is 5.261106981519217e-61\n","\n","native.country = p value is 7.804143463331444e-10\n","\n","education.num = p value is 0.0"],"metadata":{"id":"jPPtsI-84tDN"}},{"cell_type":"markdown","source":["**one Hot Enoding**"],"metadata":{"id":"wqaRD3lgA05G"}},{"cell_type":"code","source":["#categorical_features = ['workclass','marital.status','occupation','relationship','race','sex','native.country']\n","df = pd.get_dummies(df,columns=['workclass','marital.status','occupation','relationship','race','sex','native.country'],drop_first=True)"],"metadata":{"id":"Uh2dw-WLA6h0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# splitting data \n","X = df.drop('income',axis=1)\n","Y= df.income"],"metadata":{"id":"G60KY7kFCBGQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn import preprocessing\n","le = preprocessing.LabelEncoder()\n","le.fit(Y)\n","Y = le.transform(Y)\n","Y_mappings = {index: label for index, label in enumerate(le.classes_)}\n","Y_mappings"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9Ydn4x7U7r1T","executionInfo":{"status":"ok","timestamp":1639310739171,"user_tz":-330,"elapsed":472,"user":{"displayName":"Aashish Rana","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13972619416689181999"}},"outputId":"358b72a4-5e98-467a-a982-b002f3692f7c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{0: '<=50K', 1: '>50K'}"]},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","source":["**Normalization**"],"metadata":{"id":"RV74nHfpgyHh"}},{"cell_type":"code","source":["# normalizing the data\n","from sklearn.preprocessing import MinMaxScaler\n","scaler = MinMaxScaler()\n","X = pd.DataFrame(scaler.fit_transform(X),columns = X.columns)"],"metadata":{"id":"loMJ7s_gC8VJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# doing train test split and saving into dataframe\n","\n","X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.2)\n","X_train_adult = pd.DataFrame(X_train)\n","X_test_adult = pd.DataFrame(X_test)\n","Y_train_adult = pd.DataFrame(Y_train)\n","Y_test_adult = pd.DataFrame(Y_test)"],"metadata":{"id":"9KiQJJ7QMfFV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# checking the shape of the data\n","print(\"shape of original dataset :\", df.shape)\n","print(\"shape of X_train\", X_train.shape)\n","print(\"shape of Y_train\", Y_train.shape)\n","print(\"shape of X_test\", X_test.shape)\n","print(\"shape of Y_test]\", Y_test.shape)"],"metadata":{"id":"TnDs_h2Mljmu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# saving csv file\n","X_train_adult.to_csv('/content/drive/My Drive/X_train_adult.csv',index=False)\n","X_test_adult.to_csv('/content/drive/My Drive/X_test_adult.csv',index=False)\n","Y_train_adult.to_csv('/content/drive/My Drive/Y_train_adult.csv',index=False)\n","Y_test_adult.to_csv('/content/drive/My Drive/Y_test_adult.csv',index=False)"],"metadata":{"id":"UoPwZps1i6wC"},"execution_count":null,"outputs":[]}]}